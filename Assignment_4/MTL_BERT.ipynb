{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MTL_BERT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNT9o9FXjOYdRo+adhc1OiL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ranjani94/Advanced_Deep_Learning/blob/master/Assignment_4/MTL_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pSsULj7ekUU"
      },
      "source": [
        "## Defining new problem type and data reading function:Using IMDB dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xnLkmmceHTB"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZLibLtblQge",
        "outputId": "42d0324e-7ddf-4046-d887-0f49a944e62b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        }
      },
      "source": [
        "!pip install bert-multitask-learning"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-multitask-learning\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2e/20/c65f97b169f2846416a124a6b5ede1c20f160d79ff5dbeaebe18beb970c4/bert_multitask_learning-0.5.6-py3-none-any.whl (146kB)\n",
            "\r\u001b[K     |██▎                             | 10kB 27.2MB/s eta 0:00:01\r\u001b[K     |████▌                           | 20kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 30kB 7.2MB/s eta 0:00:01\r\u001b[K     |█████████                       | 40kB 7.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 51kB 6.7MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 61kB 7.3MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 71kB 8.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 81kB 8.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 92kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 102kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 112kB 8.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 122kB 8.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 133kB 8.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 143kB 8.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 8.5MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 18.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from bert-multitask-learning) (1.18.5)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from bert-multitask-learning) (3.2.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from bert-multitask-learning) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from bert-multitask-learning) (50.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from bert-multitask-learning) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-multitask-learning) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from bert-multitask-learning) (4.41.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from bert-multitask-learning) (0.22.2.post1)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 30.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers->bert-multitask-learning) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers->bert-multitask-learning) (20.4)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 56.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers->bert-multitask-learning) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers->bert-multitask-learning) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 55.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers->bert-multitask-learning) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->bert-multitask-learning) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->bert-multitask-learning) (2018.9)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->bert-multitask-learning) (1.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers->bert-multitask-learning) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->bert-multitask-learning) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->bert-multitask-learning) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->bert-multitask-learning) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->bert-multitask-learning) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->bert-multitask-learning) (2.10)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=7247ee1902f800aec643c50c0f77346fab8d4b166c0666f9865cd7d46d5fe202\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers, bert-multitask-learning\n",
            "Successfully installed bert-multitask-learning-0.5.6 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc2 transformers-3.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NevO-VYml5FF",
        "outputId": "cc3e28c6-41ec-4121-a9b8-652f44836583",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!pip install bert-tensorflow"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/16/0f9376af49c6adcfbaf2470a8f500105a74dd803aa54ac0110af445837b5/bert_tensorflow-1.0.4-py2.py3-none-any.whl (64kB)\n",
            "\r\u001b[K     |█████                           | 10kB 23.1MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 20kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 30kB 7.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 40kB 7.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 51kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 61kB 7.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from bert-tensorflow) (1.15.0)\n",
            "Installing collected packages: bert-tensorflow\n",
            "Successfully installed bert-tensorflow-1.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLsMKzMOlqCQ"
      },
      "source": [
        "import bert\n",
        "# from bert import bert_tokenization\n",
        "from bert.tokenization import FullTokenizer"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy906RWmeOrI",
        "outputId": "4b7aa663-b513-49d5-8a9a-325b33c42f8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "from bert_multitask_learning import (get_or_make_label_encoder, FullTokenizer, create_single_problem_generator, train_bert_multitask, \n",
        "                                     eval_bert_multitask, DynamicBatchSizeParams, TRAIN, EVAL, PREDICT, BertMultiTask,preprocessing_fn)\n",
        "import pickle\n",
        "import types\n",
        "import os"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-e7fd99de8884>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m from bert_multitask_learning import (get_or_make_label_encoder, FullTokenizer, create_single_problem_generator, train_bert_multitask, \n\u001b[0m\u001b[1;32m      4\u001b[0m                                      eval_bert_multitask, DynamicBatchSizeParams, TRAIN, EVAL, PREDICT, BertMultiTask,preprocessing_fn)\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'FullTokenizer'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvsI7hM6eOpL"
      },
      "source": [
        "\n",
        "new_problem_type = {'imdb_cls': 'cls'}\n",
        "\n",
        "@preprocessing_fn\n",
        "def imdb_cls(params, mode):\n",
        "\n",
        "    # get data\n",
        "    (train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=10000)\n",
        "    label_encoder = get_or_make_label_encoder(params, 'imdb_cls', mode, train_labels+test_labels)\n",
        "    word_to_id = keras.datasets.imdb.get_word_index()\n",
        "    index_from=3\n",
        "    word_to_id = {k:(v+index_from) for k,v in word_to_id.items()}\n",
        "    word_to_id[\"<PAD>\"] = 0\n",
        "    word_to_id[\"<START>\"] = 1\n",
        "    word_to_id[\"<UNK>\"] = 2\n",
        "    id_to_word = {value:key for key,value in word_to_id.items()}\n",
        "\n",
        "    train_data = [[id_to_word[i] for i in sentence] for sentence in train_data]\n",
        "    test_data = [[id_to_word[i] for i in sentence] for sentence in test_data]\n",
        "    \n",
        "    if mode == TRAIN:\n",
        "        input_list = train_data\n",
        "        target_list = train_labels\n",
        "    else:\n",
        "        input_list = test_data\n",
        "        target_list = test_labels\n",
        "    \n",
        "    return input_list, target_list\n",
        "new_problem_process_fn_dict = {'imdb_cls': imdb_cls}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rcbjuNAeOm5"
      },
      "source": [
        "params = DynamicBatchSizeParams()\n",
        "params.init_checkpoint = 'models/cased_L-12_H-768_A-12'\n",
        "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
        "train_bert_multitask(problem='imdb_cls', num_gpus=1, \n",
        "                     num_epochs=10, params=params, \n",
        "                     problem_type_dict=new_problem_type, processing_fn_dict=new_problem_process_fn_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooQVu6TeeOkL"
      },
      "source": [
        "print(eval_bert_multitask(problem='imdb_cls', num_gpus=1, \n",
        "                     params=params, eval_scheme='acc',\n",
        "                     problem_type_dict=new_problem_type, processing_fn_dict=new_problem_process_fn_dict))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}