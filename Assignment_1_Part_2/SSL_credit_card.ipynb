{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SSL_credit_card.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNLJ3mxp8MkS1sTFx38Fm/3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ranjani94/Advanced_Deep_Learning/blob/master/Assignment_1_Part_2/SSL_credit_card.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM4DiGq0IS0z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "2f9f09a9-bb23-4abe-964b-cb33d91d9917"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "root_dir = \"/content/gdrive/My Drive/\"\n",
        "base_dir = root_dir + 'advanced_DL'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/3wFNx6tRLppWk9IZ2iffX1WB6P_rKQVpCXq838dAJR6QIXQxgPsMp6A\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3cmnJUuIewR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "005ab199-46db-4154-f096-f59eb0aa9fdc"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os, time, re\n",
        "import pickle, gzip\n",
        "\n",
        "'''Data Viz'''\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "color = sns.color_palette()\n",
        "import matplotlib as mpl\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "'''Data Prep and Model Evaluation'''\n",
        "from sklearn import preprocessing as pp\n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.model_selection import StratifiedKFold \n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "\n",
        "'''Algos'''\n",
        "import lightgbm as lgb\n",
        "\n",
        "'''TensorFlow and Keras'''\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Activation, Dense, Dropout\n",
        "from keras.layers import BatchNormalization, Input, Lambda\n",
        "from keras import regularizers\n",
        "from keras.losses import mse, binary_crossentropy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFvRxhoxkjt-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Load the data\n",
        "current_path = os.getcwd()\n",
        "file = os.path.sep.join(['', '/gdrive/My Drive/advanced_DL/datasets', 'credit_card_data', 'credit.csv'])\n",
        "data = pd.read_csv(current_path + file)\n",
        "\n",
        "dataX = data.copy().drop(['Class','Time'],axis=1)\n",
        "dataY = data['Class'].copy()\n",
        "\n",
        "# Scale data\n",
        "featuresToScale = dataX.columns\n",
        "sX = pp.StandardScaler(copy=True, with_mean=True, with_std=True)\n",
        "dataX.loc[:,featuresToScale] = sX.fit_transform(dataX[featuresToScale])\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(dataX, dataY, test_size=0.33, \\\n",
        "                     random_state=2018, stratify=dataY)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mptzrryKYZer",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "a586eb13-fbc1-4eab-ad1c-4c109e12c048"
      },
      "source": [
        "dataX.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.875569</td>\n",
              "      <td>-0.272499</td>\n",
              "      <td>1.663939</td>\n",
              "      <td>0.912529</td>\n",
              "      <td>-0.290615</td>\n",
              "      <td>0.295121</td>\n",
              "      <td>0.150634</td>\n",
              "      <td>0.178994</td>\n",
              "      <td>0.398020</td>\n",
              "      <td>0.037563</td>\n",
              "      <td>-0.667131</td>\n",
              "      <td>-1.347069</td>\n",
              "      <td>-0.968644</td>\n",
              "      <td>-0.160849</td>\n",
              "      <td>1.371151</td>\n",
              "      <td>-0.435956</td>\n",
              "      <td>0.401329</td>\n",
              "      <td>0.251564</td>\n",
              "      <td>0.464223</td>\n",
              "      <td>0.333227</td>\n",
              "      <td>-0.011617</td>\n",
              "      <td>0.664810</td>\n",
              "      <td>-0.180954</td>\n",
              "      <td>0.115092</td>\n",
              "      <td>0.038888</td>\n",
              "      <td>-0.480964</td>\n",
              "      <td>0.352200</td>\n",
              "      <td>-0.037017</td>\n",
              "      <td>0.462734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.055422</td>\n",
              "      <td>0.025850</td>\n",
              "      <td>-0.705891</td>\n",
              "      <td>0.159812</td>\n",
              "      <td>0.085583</td>\n",
              "      <td>-0.162804</td>\n",
              "      <td>-0.230542</td>\n",
              "      <td>0.163926</td>\n",
              "      <td>-0.282853</td>\n",
              "      <td>-0.222083</td>\n",
              "      <td>1.501881</td>\n",
              "      <td>1.256920</td>\n",
              "      <td>0.689851</td>\n",
              "      <td>0.051381</td>\n",
              "      <td>0.445818</td>\n",
              "      <td>0.741049</td>\n",
              "      <td>-0.056118</td>\n",
              "      <td>-0.020220</td>\n",
              "      <td>-0.225337</td>\n",
              "      <td>-0.259265</td>\n",
              "      <td>-0.359887</td>\n",
              "      <td>-0.864465</td>\n",
              "      <td>0.394763</td>\n",
              "      <td>-0.544776</td>\n",
              "      <td>0.131209</td>\n",
              "      <td>0.203302</td>\n",
              "      <td>-0.069657</td>\n",
              "      <td>0.101331</td>\n",
              "      <td>-0.354937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.874469</td>\n",
              "      <td>-1.388130</td>\n",
              "      <td>0.900814</td>\n",
              "      <td>0.104472</td>\n",
              "      <td>-0.446328</td>\n",
              "      <td>1.419960</td>\n",
              "      <td>0.811298</td>\n",
              "      <td>0.344101</td>\n",
              "      <td>-1.667475</td>\n",
              "      <td>0.155262</td>\n",
              "      <td>0.511516</td>\n",
              "      <td>-0.288964</td>\n",
              "      <td>0.945487</td>\n",
              "      <td>0.023268</td>\n",
              "      <td>2.346573</td>\n",
              "      <td>-3.484147</td>\n",
              "      <td>1.679665</td>\n",
              "      <td>0.060349</td>\n",
              "      <td>-2.879436</td>\n",
              "      <td>0.838964</td>\n",
              "      <td>0.435419</td>\n",
              "      <td>1.488828</td>\n",
              "      <td>2.591810</td>\n",
              "      <td>-1.111627</td>\n",
              "      <td>-1.051296</td>\n",
              "      <td>-0.372314</td>\n",
              "      <td>-0.206890</td>\n",
              "      <td>-0.186663</td>\n",
              "      <td>1.737351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.577758</td>\n",
              "      <td>-0.371480</td>\n",
              "      <td>0.920597</td>\n",
              "      <td>-0.901635</td>\n",
              "      <td>0.019166</td>\n",
              "      <td>0.954850</td>\n",
              "      <td>0.148252</td>\n",
              "      <td>0.487909</td>\n",
              "      <td>-1.527136</td>\n",
              "      <td>-0.109245</td>\n",
              "      <td>-0.341315</td>\n",
              "      <td>-0.115455</td>\n",
              "      <td>0.710757</td>\n",
              "      <td>-0.131378</td>\n",
              "      <td>-0.962241</td>\n",
              "      <td>-1.178259</td>\n",
              "      <td>-0.862929</td>\n",
              "      <td>2.772493</td>\n",
              "      <td>-1.588511</td>\n",
              "      <td>-0.516147</td>\n",
              "      <td>-0.162687</td>\n",
              "      <td>0.210014</td>\n",
              "      <td>-0.398033</td>\n",
              "      <td>-1.900491</td>\n",
              "      <td>1.278807</td>\n",
              "      <td>-0.552243</td>\n",
              "      <td>0.142560</td>\n",
              "      <td>0.282047</td>\n",
              "      <td>0.317375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.723026</td>\n",
              "      <td>0.564207</td>\n",
              "      <td>0.676326</td>\n",
              "      <td>0.123293</td>\n",
              "      <td>-0.355659</td>\n",
              "      <td>-0.012937</td>\n",
              "      <td>0.573639</td>\n",
              "      <td>-0.230209</td>\n",
              "      <td>0.897177</td>\n",
              "      <td>0.704665</td>\n",
              "      <td>-0.938962</td>\n",
              "      <td>0.441486</td>\n",
              "      <td>1.649622</td>\n",
              "      <td>-1.185885</td>\n",
              "      <td>-0.065891</td>\n",
              "      <td>-0.412082</td>\n",
              "      <td>-0.229344</td>\n",
              "      <td>0.168418</td>\n",
              "      <td>0.965291</td>\n",
              "      <td>0.623709</td>\n",
              "      <td>0.003282</td>\n",
              "      <td>1.533211</td>\n",
              "      <td>-0.254316</td>\n",
              "      <td>0.235684</td>\n",
              "      <td>-0.760618</td>\n",
              "      <td>1.020916</td>\n",
              "      <td>0.606318</td>\n",
              "      <td>0.876379</td>\n",
              "      <td>0.019590</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         V1        V2        V3  ...       V27       V28    Amount\n",
              "0 -0.875569 -0.272499  1.663939  ...  0.352200 -0.037017  0.462734\n",
              "1  1.055422  0.025850 -0.705891  ... -0.069657  0.101331 -0.354937\n",
              "2 -0.874469 -1.388130  0.900814  ... -0.206890 -0.186663  1.737351\n",
              "3 -0.577758 -0.371480  0.920597  ...  0.142560  0.282047  0.317375\n",
              "4 -0.723026  0.564207  0.676326  ...  0.606318  0.876379  0.019590\n",
              "\n",
              "[5 rows x 29 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kU5eiMJ7YeKL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "07d6fee6-77d0-4032-9d18-6bc4e37840f7"
      },
      "source": [
        "dataY.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0\n",
              "1    0\n",
              "2    0\n",
              "3    0\n",
              "4    0\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pobU7pZ2YmUa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "a8664f07-1b5d-4d96-fba5-0ec73303c1dc"
      },
      "source": [
        "dataX.dropna(axis='columns')\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.875569</td>\n",
              "      <td>-0.272499</td>\n",
              "      <td>1.663939</td>\n",
              "      <td>0.912529</td>\n",
              "      <td>-0.290615</td>\n",
              "      <td>0.295121</td>\n",
              "      <td>0.150634</td>\n",
              "      <td>0.178994</td>\n",
              "      <td>0.398020</td>\n",
              "      <td>0.037563</td>\n",
              "      <td>-0.667131</td>\n",
              "      <td>-1.347069</td>\n",
              "      <td>-0.968644</td>\n",
              "      <td>-0.160849</td>\n",
              "      <td>1.371151</td>\n",
              "      <td>-0.435956</td>\n",
              "      <td>0.401329</td>\n",
              "      <td>0.251564</td>\n",
              "      <td>0.464223</td>\n",
              "      <td>0.333227</td>\n",
              "      <td>-0.011617</td>\n",
              "      <td>0.664810</td>\n",
              "      <td>-0.180954</td>\n",
              "      <td>0.115092</td>\n",
              "      <td>0.038888</td>\n",
              "      <td>-0.480964</td>\n",
              "      <td>0.352200</td>\n",
              "      <td>-0.037017</td>\n",
              "      <td>0.462734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.055422</td>\n",
              "      <td>0.025850</td>\n",
              "      <td>-0.705891</td>\n",
              "      <td>0.159812</td>\n",
              "      <td>0.085583</td>\n",
              "      <td>-0.162804</td>\n",
              "      <td>-0.230542</td>\n",
              "      <td>0.163926</td>\n",
              "      <td>-0.282853</td>\n",
              "      <td>-0.222083</td>\n",
              "      <td>1.501881</td>\n",
              "      <td>1.256920</td>\n",
              "      <td>0.689851</td>\n",
              "      <td>0.051381</td>\n",
              "      <td>0.445818</td>\n",
              "      <td>0.741049</td>\n",
              "      <td>-0.056118</td>\n",
              "      <td>-0.020220</td>\n",
              "      <td>-0.225337</td>\n",
              "      <td>-0.259265</td>\n",
              "      <td>-0.359887</td>\n",
              "      <td>-0.864465</td>\n",
              "      <td>0.394763</td>\n",
              "      <td>-0.544776</td>\n",
              "      <td>0.131209</td>\n",
              "      <td>0.203302</td>\n",
              "      <td>-0.069657</td>\n",
              "      <td>0.101331</td>\n",
              "      <td>-0.354937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.874469</td>\n",
              "      <td>-1.388130</td>\n",
              "      <td>0.900814</td>\n",
              "      <td>0.104472</td>\n",
              "      <td>-0.446328</td>\n",
              "      <td>1.419960</td>\n",
              "      <td>0.811298</td>\n",
              "      <td>0.344101</td>\n",
              "      <td>-1.667475</td>\n",
              "      <td>0.155262</td>\n",
              "      <td>0.511516</td>\n",
              "      <td>-0.288964</td>\n",
              "      <td>0.945487</td>\n",
              "      <td>0.023268</td>\n",
              "      <td>2.346573</td>\n",
              "      <td>-3.484147</td>\n",
              "      <td>1.679665</td>\n",
              "      <td>0.060349</td>\n",
              "      <td>-2.879436</td>\n",
              "      <td>0.838964</td>\n",
              "      <td>0.435419</td>\n",
              "      <td>1.488828</td>\n",
              "      <td>2.591810</td>\n",
              "      <td>-1.111627</td>\n",
              "      <td>-1.051296</td>\n",
              "      <td>-0.372314</td>\n",
              "      <td>-0.206890</td>\n",
              "      <td>-0.186663</td>\n",
              "      <td>1.737351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.577758</td>\n",
              "      <td>-0.371480</td>\n",
              "      <td>0.920597</td>\n",
              "      <td>-0.901635</td>\n",
              "      <td>0.019166</td>\n",
              "      <td>0.954850</td>\n",
              "      <td>0.148252</td>\n",
              "      <td>0.487909</td>\n",
              "      <td>-1.527136</td>\n",
              "      <td>-0.109245</td>\n",
              "      <td>-0.341315</td>\n",
              "      <td>-0.115455</td>\n",
              "      <td>0.710757</td>\n",
              "      <td>-0.131378</td>\n",
              "      <td>-0.962241</td>\n",
              "      <td>-1.178259</td>\n",
              "      <td>-0.862929</td>\n",
              "      <td>2.772493</td>\n",
              "      <td>-1.588511</td>\n",
              "      <td>-0.516147</td>\n",
              "      <td>-0.162687</td>\n",
              "      <td>0.210014</td>\n",
              "      <td>-0.398033</td>\n",
              "      <td>-1.900491</td>\n",
              "      <td>1.278807</td>\n",
              "      <td>-0.552243</td>\n",
              "      <td>0.142560</td>\n",
              "      <td>0.282047</td>\n",
              "      <td>0.317375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.723026</td>\n",
              "      <td>0.564207</td>\n",
              "      <td>0.676326</td>\n",
              "      <td>0.123293</td>\n",
              "      <td>-0.355659</td>\n",
              "      <td>-0.012937</td>\n",
              "      <td>0.573639</td>\n",
              "      <td>-0.230209</td>\n",
              "      <td>0.897177</td>\n",
              "      <td>0.704665</td>\n",
              "      <td>-0.938962</td>\n",
              "      <td>0.441486</td>\n",
              "      <td>1.649622</td>\n",
              "      <td>-1.185885</td>\n",
              "      <td>-0.065891</td>\n",
              "      <td>-0.412082</td>\n",
              "      <td>-0.229344</td>\n",
              "      <td>0.168418</td>\n",
              "      <td>0.965291</td>\n",
              "      <td>0.623709</td>\n",
              "      <td>0.003282</td>\n",
              "      <td>1.533211</td>\n",
              "      <td>-0.254316</td>\n",
              "      <td>0.235684</td>\n",
              "      <td>-0.760618</td>\n",
              "      <td>1.020916</td>\n",
              "      <td>0.606318</td>\n",
              "      <td>0.876379</td>\n",
              "      <td>0.019590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1321</th>\n",
              "      <td>-0.480608</td>\n",
              "      <td>0.631975</td>\n",
              "      <td>0.154759</td>\n",
              "      <td>0.345060</td>\n",
              "      <td>0.298943</td>\n",
              "      <td>0.152727</td>\n",
              "      <td>1.114815</td>\n",
              "      <td>-0.079243</td>\n",
              "      <td>-0.138208</td>\n",
              "      <td>-0.311515</td>\n",
              "      <td>-1.053429</td>\n",
              "      <td>0.171080</td>\n",
              "      <td>0.614042</td>\n",
              "      <td>-0.102447</td>\n",
              "      <td>-0.665088</td>\n",
              "      <td>-0.808342</td>\n",
              "      <td>0.192387</td>\n",
              "      <td>-0.769814</td>\n",
              "      <td>-0.114324</td>\n",
              "      <td>-0.590482</td>\n",
              "      <td>0.044700</td>\n",
              "      <td>0.668841</td>\n",
              "      <td>-0.158495</td>\n",
              "      <td>-0.398854</td>\n",
              "      <td>-0.693538</td>\n",
              "      <td>-1.020806</td>\n",
              "      <td>-0.536740</td>\n",
              "      <td>0.676098</td>\n",
              "      <td>0.053759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1322</th>\n",
              "      <td>-0.337334</td>\n",
              "      <td>0.703501</td>\n",
              "      <td>1.656165</td>\n",
              "      <td>-0.024739</td>\n",
              "      <td>1.013510</td>\n",
              "      <td>0.325874</td>\n",
              "      <td>1.624057</td>\n",
              "      <td>-1.107313</td>\n",
              "      <td>0.155628</td>\n",
              "      <td>1.378972</td>\n",
              "      <td>1.096680</td>\n",
              "      <td>0.140437</td>\n",
              "      <td>1.193426</td>\n",
              "      <td>-1.148009</td>\n",
              "      <td>0.676945</td>\n",
              "      <td>0.045994</td>\n",
              "      <td>-1.873090</td>\n",
              "      <td>0.917115</td>\n",
              "      <td>1.365481</td>\n",
              "      <td>0.916257</td>\n",
              "      <td>-0.600509</td>\n",
              "      <td>-0.252817</td>\n",
              "      <td>-1.112580</td>\n",
              "      <td>-0.834810</td>\n",
              "      <td>0.027797</td>\n",
              "      <td>-1.446829</td>\n",
              "      <td>-3.044817</td>\n",
              "      <td>-3.789553</td>\n",
              "      <td>-0.342082</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1323</th>\n",
              "      <td>1.007516</td>\n",
              "      <td>-0.196576</td>\n",
              "      <td>-0.127763</td>\n",
              "      <td>0.961936</td>\n",
              "      <td>-0.606719</td>\n",
              "      <td>-0.341642</td>\n",
              "      <td>-0.427185</td>\n",
              "      <td>0.207897</td>\n",
              "      <td>0.473766</td>\n",
              "      <td>0.032445</td>\n",
              "      <td>0.695987</td>\n",
              "      <td>0.495537</td>\n",
              "      <td>-1.495092</td>\n",
              "      <td>0.632201</td>\n",
              "      <td>-1.413772</td>\n",
              "      <td>-0.111343</td>\n",
              "      <td>-0.038226</td>\n",
              "      <td>0.458994</td>\n",
              "      <td>0.185664</td>\n",
              "      <td>-0.556437</td>\n",
              "      <td>0.006157</td>\n",
              "      <td>0.415532</td>\n",
              "      <td>-0.142898</td>\n",
              "      <td>0.855024</td>\n",
              "      <td>1.257175</td>\n",
              "      <td>-0.716508</td>\n",
              "      <td>0.033670</td>\n",
              "      <td>0.089380</td>\n",
              "      <td>-0.314535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1324</th>\n",
              "      <td>0.881376</td>\n",
              "      <td>-0.667493</td>\n",
              "      <td>-0.596074</td>\n",
              "      <td>0.257718</td>\n",
              "      <td>-0.774209</td>\n",
              "      <td>-0.819509</td>\n",
              "      <td>-0.018776</td>\n",
              "      <td>-0.084571</td>\n",
              "      <td>0.656183</td>\n",
              "      <td>-0.324025</td>\n",
              "      <td>-0.814917</td>\n",
              "      <td>-0.797029</td>\n",
              "      <td>-1.244153</td>\n",
              "      <td>0.658411</td>\n",
              "      <td>0.245824</td>\n",
              "      <td>0.129310</td>\n",
              "      <td>0.103935</td>\n",
              "      <td>-0.435399</td>\n",
              "      <td>0.200519</td>\n",
              "      <td>0.106068</td>\n",
              "      <td>-0.265515</td>\n",
              "      <td>-1.103275</td>\n",
              "      <td>-0.034963</td>\n",
              "      <td>0.652481</td>\n",
              "      <td>0.276876</td>\n",
              "      <td>0.871089</td>\n",
              "      <td>-0.281217</td>\n",
              "      <td>0.178162</td>\n",
              "      <td>0.509369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1325</th>\n",
              "      <td>-0.339846</td>\n",
              "      <td>-0.424815</td>\n",
              "      <td>0.479420</td>\n",
              "      <td>-2.167310</td>\n",
              "      <td>-0.489947</td>\n",
              "      <td>-0.298384</td>\n",
              "      <td>0.699870</td>\n",
              "      <td>-0.015176</td>\n",
              "      <td>1.331332</td>\n",
              "      <td>-2.037181</td>\n",
              "      <td>-1.280730</td>\n",
              "      <td>1.143379</td>\n",
              "      <td>2.035944</td>\n",
              "      <td>-0.667256</td>\n",
              "      <td>0.313605</td>\n",
              "      <td>0.073885</td>\n",
              "      <td>-0.974254</td>\n",
              "      <td>0.782863</td>\n",
              "      <td>0.410981</td>\n",
              "      <td>0.538164</td>\n",
              "      <td>0.514012</td>\n",
              "      <td>1.604238</td>\n",
              "      <td>0.228119</td>\n",
              "      <td>-0.591362</td>\n",
              "      <td>-0.363497</td>\n",
              "      <td>-0.215381</td>\n",
              "      <td>0.300941</td>\n",
              "      <td>0.665957</td>\n",
              "      <td>0.509369</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1326 rows Ã— 29 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            V1        V2        V3  ...       V27       V28    Amount\n",
              "0    -0.875569 -0.272499  1.663939  ...  0.352200 -0.037017  0.462734\n",
              "1     1.055422  0.025850 -0.705891  ... -0.069657  0.101331 -0.354937\n",
              "2    -0.874469 -1.388130  0.900814  ... -0.206890 -0.186663  1.737351\n",
              "3    -0.577758 -0.371480  0.920597  ...  0.142560  0.282047  0.317375\n",
              "4    -0.723026  0.564207  0.676326  ...  0.606318  0.876379  0.019590\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "1321 -0.480608  0.631975  0.154759  ... -0.536740  0.676098  0.053759\n",
              "1322 -0.337334  0.703501  1.656165  ... -3.044817 -3.789553 -0.342082\n",
              "1323  1.007516 -0.196576 -0.127763  ...  0.033670  0.089380 -0.314535\n",
              "1324  0.881376 -0.667493 -0.596074  ... -0.281217  0.178162  0.509369\n",
              "1325 -0.339846 -0.424815  0.479420  ...  0.300941  0.665957  0.509369\n",
              "\n",
              "[1326 rows x 29 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDYLRx83Yz6h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "aae9ab85-455c-4a49-de99-4145cc3bfa07"
      },
      "source": [
        "dataY.dropna()\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       0\n",
              "1       0\n",
              "2       0\n",
              "3       0\n",
              "4       0\n",
              "       ..\n",
              "1321    0\n",
              "1322    0\n",
              "1323    0\n",
              "1324    0\n",
              "1325    0\n",
              "Name: Class, Length: 1326, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e6d4Wn75hux",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "55a414ba-675d-4db6-bf9d-7189687f966f"
      },
      "source": [
        "# Drop 95% of the labels from the training set\n",
        "toDrop = y_train[y_train==1].sample(frac=0.90,random_state=2018)\n",
        "X_train.drop(labels=toDrop.index,inplace=True)\n",
        "y_train.drop(labels=toDrop.index,inplace=True)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BRBlziATIrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def anomalyScores(originalDF, reducedDF):\n",
        "    loss = np.sum((np.array(originalDF) - \\\n",
        "                   np.array(reducedDF))**2, axis=1)\n",
        "    loss = pd.Series(data=loss,index=originalDF.index)\n",
        "    loss = (loss-np.min(loss))/(np.max(loss)-np.min(loss))\n",
        "    return loss"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GFNTjGqTIxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plotResults(trueLabels, anomalyScores, returnPreds = False):\n",
        "    preds = pd.concat([trueLabels, anomalyScores], axis=1)\n",
        "    preds.columns = ['trueLabel', 'anomalyScore']\n",
        "    precision, recall, thresholds = \\\n",
        "        precision_recall_curve(preds['trueLabel'], \\\n",
        "                               preds['anomalyScore'])\n",
        "    average_precision = average_precision_score( \\\n",
        "                        preds['trueLabel'], preds['anomalyScore'])\n",
        "    \n",
        "    plt.step(recall, precision, color='k', alpha=0.7, where='post')\n",
        "    plt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n",
        "\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    \n",
        "    plt.title('Precision-Recall curve: Average Precision = \\\n",
        "        {0:0.2f}'.format(average_precision))\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(preds['trueLabel'], \\\n",
        "                                     preds['anomalyScore'])\n",
        "    areaUnderROC = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\n",
        "    plt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic: Area under the \\\n",
        "        curve = {0:0.2f}'.format(areaUnderROC))\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    \n",
        "    if returnPreds==True:\n",
        "        return preds, average_precision"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjRY56koTJEV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def precisionAnalysis(df, column, threshold):\n",
        "    df.sort_values(by=column, ascending=False, inplace=True)\n",
        "    threshold_value = threshold*df.trueLabel.sum()\n",
        "    i = 0\n",
        "    j = 0\n",
        "    while i < threshold_value+1:\n",
        "        if df.iloc[j][\"trueLabel\"]==1:\n",
        "            i += 1\n",
        "        j += 1\n",
        "    return df, i/j"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjkrvgjNTJCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k_fold = StratifiedKFold(n_splits=5,shuffle=True,random_state=2018)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8j2Tvl-TIuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "params_lightGB = {\n",
        "    'task': 'train',\n",
        "    'application':'binary',\n",
        "    'num_class':1,\n",
        "    'boosting': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'metric_freq':50,\n",
        "    'is_training_metric':False,\n",
        "    'max_depth':4,\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.01,\n",
        "    'feature_fraction': 1.0,\n",
        "    'bagging_fraction': 1.0,\n",
        "    'bagging_freq': 0,\n",
        "    'bagging_seed': 2018,\n",
        "    'verbose': 0,\n",
        "    'num_threads':16\n",
        "}\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-w51KaRTIh_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cbf85ff3-9655-4dcb-97ff-24311f069197"
      },
      "source": [
        "trainingScores = []\n",
        "cvScores = []\n",
        "predictionsBasedOnKFolds = pd.DataFrame(data=[], index=y_train.index, \\\n",
        "                                        columns=['prediction'])\n",
        "\n",
        "for train_index, cv_index in k_fold.split(np.zeros(len(X_train)), \\\n",
        "                                          y_train.ravel()):\n",
        "    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n",
        "        X_train.iloc[cv_index,:]\n",
        "    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n",
        "        y_train.iloc[cv_index]\n",
        "    \n",
        "    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)\n",
        "    lgb_eval = lgb.Dataset(X_cv_fold, y_cv_fold, reference=lgb_train)\n",
        "    gbm = lgb.train(params_lightGB, lgb_train, num_boost_round=2000,\n",
        "                   valid_sets=lgb_eval, early_stopping_rounds=200)\n",
        "    \n",
        "    loglossTraining = log_loss(y_train_fold, gbm.predict(X_train_fold, \\\n",
        "                                num_iteration=gbm.best_iteration))\n",
        "    trainingScores.append(loglossTraining)\n",
        "    \n",
        "    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = \\\n",
        "        gbm.predict(X_cv_fold, num_iteration=gbm.best_iteration) \n",
        "    loglossCV = log_loss(y_cv_fold, \\\n",
        "        predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])\n",
        "    cvScores.append(loglossCV)\n",
        "    \n",
        "    print('Training Log Loss: ', loglossTraining)\n",
        "    print('CV Log Loss: ', loglossCV)\n",
        "    \n",
        "loglossLightGBMGradientBoosting = log_loss(y_train, \\\n",
        "        predictionsBasedOnKFolds.loc[:,'prediction'])\n",
        "print('LightGBM Gradient Boosting Log Loss: ', \\\n",
        "        loglossLightGBMGradientBoosting)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1]\tvalid_0's binary_logloss: 0\n",
            "Training until validation scores don't improve for 200 rounds.\n",
            "[2]\tvalid_0's binary_logloss: 0\n",
            "[3]\tvalid_0's binary_logloss: 0\n",
            "[4]\tvalid_0's binary_logloss: 0\n",
            "[5]\tvalid_0's binary_logloss: 0\n",
            "[6]\tvalid_0's binary_logloss: 0\n",
            "[7]\tvalid_0's binary_logloss: 0\n",
            "[8]\tvalid_0's binary_logloss: 0\n",
            "[9]\tvalid_0's binary_logloss: 0\n",
            "[10]\tvalid_0's binary_logloss: 0\n",
            "[11]\tvalid_0's binary_logloss: 0\n",
            "[12]\tvalid_0's binary_logloss: 0\n",
            "[13]\tvalid_0's binary_logloss: 0\n",
            "[14]\tvalid_0's binary_logloss: 0\n",
            "[15]\tvalid_0's binary_logloss: 0\n",
            "[16]\tvalid_0's binary_logloss: 0\n",
            "[17]\tvalid_0's binary_logloss: 0\n",
            "[18]\tvalid_0's binary_logloss: 0\n",
            "[19]\tvalid_0's binary_logloss: 0\n",
            "[20]\tvalid_0's binary_logloss: 0\n",
            "[21]\tvalid_0's binary_logloss: 0\n",
            "[22]\tvalid_0's binary_logloss: 0\n",
            "[23]\tvalid_0's binary_logloss: 0\n",
            "[24]\tvalid_0's binary_logloss: 0\n",
            "[25]\tvalid_0's binary_logloss: 0\n",
            "[26]\tvalid_0's binary_logloss: 0\n",
            "[27]\tvalid_0's binary_logloss: 0\n",
            "[28]\tvalid_0's binary_logloss: 0\n",
            "[29]\tvalid_0's binary_logloss: 0\n",
            "[30]\tvalid_0's binary_logloss: 0\n",
            "[31]\tvalid_0's binary_logloss: 0\n",
            "[32]\tvalid_0's binary_logloss: 0\n",
            "[33]\tvalid_0's binary_logloss: 0\n",
            "[34]\tvalid_0's binary_logloss: 0\n",
            "[35]\tvalid_0's binary_logloss: 0\n",
            "[36]\tvalid_0's binary_logloss: 0\n",
            "[37]\tvalid_0's binary_logloss: 0\n",
            "[38]\tvalid_0's binary_logloss: 0\n",
            "[39]\tvalid_0's binary_logloss: 0\n",
            "[40]\tvalid_0's binary_logloss: 0\n",
            "[41]\tvalid_0's binary_logloss: 0\n",
            "[42]\tvalid_0's binary_logloss: 0\n",
            "[43]\tvalid_0's binary_logloss: 0\n",
            "[44]\tvalid_0's binary_logloss: 0\n",
            "[45]\tvalid_0's binary_logloss: 0\n",
            "[46]\tvalid_0's binary_logloss: 0\n",
            "[47]\tvalid_0's binary_logloss: 0\n",
            "[48]\tvalid_0's binary_logloss: 0\n",
            "[49]\tvalid_0's binary_logloss: 0\n",
            "[50]\tvalid_0's binary_logloss: 0\n",
            "[51]\tvalid_0's binary_logloss: 0\n",
            "[52]\tvalid_0's binary_logloss: 0\n",
            "[53]\tvalid_0's binary_logloss: 0\n",
            "[54]\tvalid_0's binary_logloss: 0\n",
            "[55]\tvalid_0's binary_logloss: 0\n",
            "[56]\tvalid_0's binary_logloss: 0\n",
            "[57]\tvalid_0's binary_logloss: 0\n",
            "[58]\tvalid_0's binary_logloss: 0\n",
            "[59]\tvalid_0's binary_logloss: 0\n",
            "[60]\tvalid_0's binary_logloss: 0\n",
            "[61]\tvalid_0's binary_logloss: 0\n",
            "[62]\tvalid_0's binary_logloss: 0\n",
            "[63]\tvalid_0's binary_logloss: 0\n",
            "[64]\tvalid_0's binary_logloss: 0\n",
            "[65]\tvalid_0's binary_logloss: 0\n",
            "[66]\tvalid_0's binary_logloss: 0\n",
            "[67]\tvalid_0's binary_logloss: 0\n",
            "[68]\tvalid_0's binary_logloss: 0\n",
            "[69]\tvalid_0's binary_logloss: 0\n",
            "[70]\tvalid_0's binary_logloss: 0\n",
            "[71]\tvalid_0's binary_logloss: 0\n",
            "[72]\tvalid_0's binary_logloss: 0\n",
            "[73]\tvalid_0's binary_logloss: 0\n",
            "[74]\tvalid_0's binary_logloss: 0\n",
            "[75]\tvalid_0's binary_logloss: 0\n",
            "[76]\tvalid_0's binary_logloss: 0\n",
            "[77]\tvalid_0's binary_logloss: 0\n",
            "[78]\tvalid_0's binary_logloss: 0\n",
            "[79]\tvalid_0's binary_logloss: 0\n",
            "[80]\tvalid_0's binary_logloss: 0\n",
            "[81]\tvalid_0's binary_logloss: 0\n",
            "[82]\tvalid_0's binary_logloss: 0\n",
            "[83]\tvalid_0's binary_logloss: 0\n",
            "[84]\tvalid_0's binary_logloss: 0\n",
            "[85]\tvalid_0's binary_logloss: 0\n",
            "[86]\tvalid_0's binary_logloss: 0\n",
            "[87]\tvalid_0's binary_logloss: 0\n",
            "[88]\tvalid_0's binary_logloss: 0\n",
            "[89]\tvalid_0's binary_logloss: 0\n",
            "[90]\tvalid_0's binary_logloss: 0\n",
            "[91]\tvalid_0's binary_logloss: 0\n",
            "[92]\tvalid_0's binary_logloss: 0\n",
            "[93]\tvalid_0's binary_logloss: 0\n",
            "[94]\tvalid_0's binary_logloss: 0\n",
            "[95]\tvalid_0's binary_logloss: 0\n",
            "[96]\tvalid_0's binary_logloss: 0\n",
            "[97]\tvalid_0's binary_logloss: 0\n",
            "[98]\tvalid_0's binary_logloss: 0\n",
            "[99]\tvalid_0's binary_logloss: 0\n",
            "[100]\tvalid_0's binary_logloss: 0\n",
            "[101]\tvalid_0's binary_logloss: 0\n",
            "[102]\tvalid_0's binary_logloss: 0\n",
            "[103]\tvalid_0's binary_logloss: 0\n",
            "[104]\tvalid_0's binary_logloss: 0\n",
            "[105]\tvalid_0's binary_logloss: 0\n",
            "[106]\tvalid_0's binary_logloss: 0\n",
            "[107]\tvalid_0's binary_logloss: 0\n",
            "[108]\tvalid_0's binary_logloss: 0\n",
            "[109]\tvalid_0's binary_logloss: 0\n",
            "[110]\tvalid_0's binary_logloss: 0\n",
            "[111]\tvalid_0's binary_logloss: 0\n",
            "[112]\tvalid_0's binary_logloss: 0\n",
            "[113]\tvalid_0's binary_logloss: 0\n",
            "[114]\tvalid_0's binary_logloss: 0\n",
            "[115]\tvalid_0's binary_logloss: 0\n",
            "[116]\tvalid_0's binary_logloss: 0\n",
            "[117]\tvalid_0's binary_logloss: 0\n",
            "[118]\tvalid_0's binary_logloss: 0\n",
            "[119]\tvalid_0's binary_logloss: 0\n",
            "[120]\tvalid_0's binary_logloss: 0\n",
            "[121]\tvalid_0's binary_logloss: 0\n",
            "[122]\tvalid_0's binary_logloss: 0\n",
            "[123]\tvalid_0's binary_logloss: 0\n",
            "[124]\tvalid_0's binary_logloss: 0\n",
            "[125]\tvalid_0's binary_logloss: 0\n",
            "[126]\tvalid_0's binary_logloss: 0\n",
            "[127]\tvalid_0's binary_logloss: 0\n",
            "[128]\tvalid_0's binary_logloss: 0\n",
            "[129]\tvalid_0's binary_logloss: 0\n",
            "[130]\tvalid_0's binary_logloss: 0\n",
            "[131]\tvalid_0's binary_logloss: 0\n",
            "[132]\tvalid_0's binary_logloss: 0\n",
            "[133]\tvalid_0's binary_logloss: 0\n",
            "[134]\tvalid_0's binary_logloss: 0\n",
            "[135]\tvalid_0's binary_logloss: 0\n",
            "[136]\tvalid_0's binary_logloss: 0\n",
            "[137]\tvalid_0's binary_logloss: 0\n",
            "[138]\tvalid_0's binary_logloss: 0\n",
            "[139]\tvalid_0's binary_logloss: 0\n",
            "[140]\tvalid_0's binary_logloss: 0\n",
            "[141]\tvalid_0's binary_logloss: 0\n",
            "[142]\tvalid_0's binary_logloss: 0\n",
            "[143]\tvalid_0's binary_logloss: 0\n",
            "[144]\tvalid_0's binary_logloss: 0\n",
            "[145]\tvalid_0's binary_logloss: 0\n",
            "[146]\tvalid_0's binary_logloss: 0\n",
            "[147]\tvalid_0's binary_logloss: 0\n",
            "[148]\tvalid_0's binary_logloss: 0\n",
            "[149]\tvalid_0's binary_logloss: 0\n",
            "[150]\tvalid_0's binary_logloss: 0\n",
            "[151]\tvalid_0's binary_logloss: 0\n",
            "[152]\tvalid_0's binary_logloss: 0\n",
            "[153]\tvalid_0's binary_logloss: 0\n",
            "[154]\tvalid_0's binary_logloss: 0\n",
            "[155]\tvalid_0's binary_logloss: 0\n",
            "[156]\tvalid_0's binary_logloss: 0\n",
            "[157]\tvalid_0's binary_logloss: 0\n",
            "[158]\tvalid_0's binary_logloss: 0\n",
            "[159]\tvalid_0's binary_logloss: 0\n",
            "[160]\tvalid_0's binary_logloss: 0\n",
            "[161]\tvalid_0's binary_logloss: 0\n",
            "[162]\tvalid_0's binary_logloss: 0\n",
            "[163]\tvalid_0's binary_logloss: 0\n",
            "[164]\tvalid_0's binary_logloss: 0\n",
            "[165]\tvalid_0's binary_logloss: 0\n",
            "[166]\tvalid_0's binary_logloss: 0\n",
            "[167]\tvalid_0's binary_logloss: 0\n",
            "[168]\tvalid_0's binary_logloss: 0\n",
            "[169]\tvalid_0's binary_logloss: 0\n",
            "[170]\tvalid_0's binary_logloss: 0\n",
            "[171]\tvalid_0's binary_logloss: 0\n",
            "[172]\tvalid_0's binary_logloss: 0\n",
            "[173]\tvalid_0's binary_logloss: 0\n",
            "[174]\tvalid_0's binary_logloss: 0\n",
            "[175]\tvalid_0's binary_logloss: 0\n",
            "[176]\tvalid_0's binary_logloss: 0\n",
            "[177]\tvalid_0's binary_logloss: 0\n",
            "[178]\tvalid_0's binary_logloss: 0\n",
            "[179]\tvalid_0's binary_logloss: 0\n",
            "[180]\tvalid_0's binary_logloss: 0\n",
            "[181]\tvalid_0's binary_logloss: 0\n",
            "[182]\tvalid_0's binary_logloss: 0\n",
            "[183]\tvalid_0's binary_logloss: 0\n",
            "[184]\tvalid_0's binary_logloss: 0\n",
            "[185]\tvalid_0's binary_logloss: 0\n",
            "[186]\tvalid_0's binary_logloss: 0\n",
            "[187]\tvalid_0's binary_logloss: 0\n",
            "[188]\tvalid_0's binary_logloss: 0\n",
            "[189]\tvalid_0's binary_logloss: 0\n",
            "[190]\tvalid_0's binary_logloss: 0\n",
            "[191]\tvalid_0's binary_logloss: 0\n",
            "[192]\tvalid_0's binary_logloss: 0\n",
            "[193]\tvalid_0's binary_logloss: 0\n",
            "[194]\tvalid_0's binary_logloss: 0\n",
            "[195]\tvalid_0's binary_logloss: 0\n",
            "[196]\tvalid_0's binary_logloss: 0\n",
            "[197]\tvalid_0's binary_logloss: 0\n",
            "[198]\tvalid_0's binary_logloss: 0\n",
            "[199]\tvalid_0's binary_logloss: 0\n",
            "[200]\tvalid_0's binary_logloss: 0\n",
            "[201]\tvalid_0's binary_logloss: 0\n",
            "Early stopping, best iteration is:\n",
            "[1]\tvalid_0's binary_logloss: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-cb29e6613474>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                    valid_sets=lgb_eval, early_stopping_rounds=200)\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mloglossTraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_fold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_fold\u001b[0m\u001b[0;34m,\u001b[0m                                 \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtrainingScores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloglossTraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mlog_loss\u001b[0;34m(y_true, y_pred, eps, normalize, sample_weight, labels)\u001b[0m\n\u001b[1;32m   2253\u001b[0m             raise ValueError('y_true contains only one label ({0}). Please '\n\u001b[1;32m   2254\u001b[0m                              \u001b[0;34m'provide the true labels explicitly through the '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2255\u001b[0;31m                              'labels argument.'.format(lb.classes_[0]))\n\u001b[0m\u001b[1;32m   2256\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2257\u001b[0m             raise ValueError('The labels array needs to contain at least two '\n",
            "\u001b[0;31mValueError\u001b[0m: y_true contains only one label (0). Please provide the true labels explicitly through the labels argument."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRDVN3pmTYzt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "9a663436-f158-4342-d4c4-c46399ad7506"
      },
      "source": [
        "\n",
        "preds, average_precision = plotResults(y_train, predictionsBasedOnKFolds.loc[:,'prediction'], True)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-a34ccd53bbba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage_precision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplotResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictionsBasedOnKFolds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'prediction'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-ea4ee24be441>\u001b[0m in \u001b[0;36mplotResults\u001b[0;34m(trueLabels, anomalyScores, returnPreds)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrueLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manomalyScores\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'trueLabel'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'anomalyScore'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m         \u001b[0mprecision_recall_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trueLabel'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m                                \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'anomalyScore'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0maverage_precision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maverage_precision_score\u001b[0m\u001b[0;34m(\u001b[0m                         \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trueLabel'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'anomalyScore'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mprecision_recall_curve\u001b[0;34m(y_true, probas_pred, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    671\u001b[0m     fps, tps, thresholds = _binary_clf_curve(y_true, probas_pred,\n\u001b[1;32m    672\u001b[0m                                              \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m                                              sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtps\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtps\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0massert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m     \u001b[0massert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36massert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mallow_nan\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \"\"\"\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Input contains NaN\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input contains NaN"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrX6xTsvTY4f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "7b955053-e0bf-4133-c2a7-e35da18da328"
      },
      "source": [
        "predictions = pd.Series(data=gbm.predict(X_test, \\\n",
        "                    num_iteration=gbm.best_iteration),index=X_test.index)\n",
        "preds, average_precision = plotResults(y_test, predictions, True)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCI5lbPWTYxp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        },
        "outputId": "106be83e-9932-49fe-c9b0-3004f913388e"
      },
      "source": [
        "preds, precision = precisionAnalysis(preds, \"anomalyScore\", 0.75)\n",
        "print(precision)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-8fa41dba50ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecisionAnalysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"anomalyScore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-b4fe15a9152f>\u001b[0m in \u001b[0;36mprecisionAnalysis\u001b[0;34m(df, column, threshold)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mthreshold_value\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"trueLabel\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mj\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2138\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2061\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2062\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2063\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUCm0CEuTqmn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "outputId": "457c9203-c926-4372-b972-13144dde6cd0"
      },
      "source": [
        "featuresImportance = pd.DataFrame(data=list(gbm.feature_importance()), \\\n",
        "                        index=X_train.columns,columns=['featImportance'])\n",
        "featuresImportance = featuresImportance/featuresImportance.sum()\n",
        "featuresImportance.sort_values(by='featImportance', \\\n",
        "                               ascending=False,inplace=True)\n",
        "featuresImportance"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>featImportance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>V1</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V2</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V3</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V4</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V5</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V6</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V7</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V8</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V9</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V10</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V11</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V12</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V13</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V14</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V15</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V16</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V17</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V18</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V19</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V20</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V21</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V22</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V23</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V24</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V25</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V26</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V27</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>V28</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Amount</th>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        featImportance\n",
              "V1                 NaN\n",
              "V2                 NaN\n",
              "V3                 NaN\n",
              "V4                 NaN\n",
              "V5                 NaN\n",
              "V6                 NaN\n",
              "V7                 NaN\n",
              "V8                 NaN\n",
              "V9                 NaN\n",
              "V10                NaN\n",
              "V11                NaN\n",
              "V12                NaN\n",
              "V13                NaN\n",
              "V14                NaN\n",
              "V15                NaN\n",
              "V16                NaN\n",
              "V17                NaN\n",
              "V18                NaN\n",
              "V19                NaN\n",
              "V20                NaN\n",
              "V21                NaN\n",
              "V22                NaN\n",
              "V23                NaN\n",
              "V24                NaN\n",
              "V25                NaN\n",
              "V26                NaN\n",
              "V27                NaN\n",
              "V28                NaN\n",
              "Amount             NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E4XbNvuZAm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}