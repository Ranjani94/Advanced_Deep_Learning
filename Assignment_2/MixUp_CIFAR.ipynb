{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MixUp_CIFAR.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM2sg2Ul7ZlW4ZgI/d5H41z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ranjani94/Advanced_Deep_Learning/blob/master/Assignment_2/MixUp_CIFAR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8QIk1RHB8DS",
        "colab_type": "text"
      },
      "source": [
        "##FastAI Mixup Augmentation, Label smoothing, Test Time Augmentation, Progressive Resizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Voc28ljKky9",
        "colab_type": "text"
      },
      "source": [
        "Importing Fastbook. These notebooks cover an introduction to deep learning, fastai, and PyTorch. fastai is a layered API for deep learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df3tOoIqBzDo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "e4cc5bf6-aa4c-4958-bcd2-2e473204dca8"
      },
      "source": [
        "!pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 727kB 4.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 6.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 19.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 358kB 28.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 92kB 9.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 40kB 6.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 40kB 6.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 8.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 9.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.6MB 31.1MB/s \n",
            "\u001b[?25hMounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhqlWikxCdx4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastbook import *"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls1sUl7xLRK5",
        "colab_type": "text"
      },
      "source": [
        "###Using CIFAR 10 dataset for mixup augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qx_D_8dRC7EB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "937cad16-4bb6-47ae-b16d-cf887ccebdcf"
      },
      "source": [
        "from fastai.vision.all import *\n",
        "path = untar_data(URLs.CIFAR)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFKC8BOgLZhg",
        "colab_type": "text"
      },
      "source": [
        "###Loading data from FastAi library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xpyY3wZDEP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dblock = DataBlock(blocks=(ImageBlock(), CategoryBlock()),\n",
        "                   get_items=get_image_files,\n",
        "                   get_y=parent_label,\n",
        "                   item_tfms=Resize(460),\n",
        "                   batch_tfms=aug_transforms(size=224, min_scale=0.75))\n",
        "dls = dblock.dataloaders(path, bs=64)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGkyFVM5Le6X",
        "colab_type": "text"
      },
      "source": [
        "###Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVDhDjQ4LmyS",
        "colab_type": "text"
      },
      "source": [
        "###Training CIFAR dataset using pre trained model ResNet50, loss function as cross Entropy\n",
        "\n",
        "When working with models that are being trained from scratch, or fine-tuned to a very different dataset than the one used for the pretraining, there are some additional techniques that are really important. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuBR1WYPDy_v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bc49ff26-99e6-4044-865f-713c706de307"
      },
      "source": [
        "model = xresnet50()\n",
        "learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n",
        "learn.fit_one_cycle(5, 3e-3)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.430894</td>\n",
              "      <td>1.906714</td>\n",
              "      <td>0.401500</td>\n",
              "      <td>11:16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.033549</td>\n",
              "      <td>1.046034</td>\n",
              "      <td>0.629833</td>\n",
              "      <td>11:22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.793301</td>\n",
              "      <td>0.843580</td>\n",
              "      <td>0.711500</td>\n",
              "      <td>11:22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.620378</td>\n",
              "      <td>0.583736</td>\n",
              "      <td>0.796417</td>\n",
              "      <td>11:22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.527582</td>\n",
              "      <td>0.512798</td>\n",
              "      <td>0.824250</td>\n",
              "      <td>11:22</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAC6hNAKEL8t",
        "colab_type": "text"
      },
      "source": [
        "###Normalization\n",
        "\n",
        "Normalizing the training data is very important in order to reduce bias in dataset ( mean 0 and standard deviation 1), but in computer vision the values are between 0 and 255 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_E0dl8SD3AH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0404e4da-cf9a-41f0-d528-103209d1ade2"
      },
      "source": [
        "x,y = dls.one_batch()\n",
        "x.mean(dim=[0,2,3]),x.std(dim=[0,2,3])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorImage([0.4828, 0.4759, 0.4275], device='cuda:0'),\n",
              " TensorImage([0.2182, 0.2161, 0.2406], device='cuda:0'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvKPiqIMNAUj",
        "colab_type": "text"
      },
      "source": [
        "The mean and standard deviation are not very close to the desired values. Fortunately, normalizing the data is easy to do in fastai by adding the Normalize transform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36-Cx9fAD9km",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dls(bs, size):\n",
        "    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
        "                   get_items=get_image_files,\n",
        "                   get_y=parent_label,\n",
        "                   item_tfms=Resize(460),\n",
        "                   batch_tfms=[*aug_transforms(size=size, min_scale=0.75),\n",
        "                               Normalize.from_stats(*imagenet_stats)])\n",
        "    return dblock.dataloaders(path, bs=bs)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAL-PDr4D-zz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dls = get_dls(64, 224)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sScI6jJaD_GH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3dd9778b-295d-478d-fe62-b7550595ff35"
      },
      "source": [
        "x,y = dls.one_batch()\n",
        "x.mean(dim=[0,2,3]),x.std(dim=[0,2,3])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorImage([-0.0640, -0.0176,  0.0812], device='cuda:0'),\n",
              " TensorImage([1.0912, 1.0718, 1.1660], device='cuda:0'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su_SHAmoNI2X",
        "colab_type": "text"
      },
      "source": [
        "After Normalization, the effects on our model evaluation is shown"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1SOGk1xD-4i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "8831cf3a-c1ae-49d4-ba10-131c6363a6e5"
      },
      "source": [
        "model = xresnet50()\n",
        "learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy)\n",
        "learn.fit_one_cycle(5, 3e-3)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.432934</td>\n",
              "      <td>1.313992</td>\n",
              "      <td>0.527583</td>\n",
              "      <td>11:24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.001459</td>\n",
              "      <td>0.868518</td>\n",
              "      <td>0.691000</td>\n",
              "      <td>11:23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.785417</td>\n",
              "      <td>0.691331</td>\n",
              "      <td>0.758167</td>\n",
              "      <td>11:23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.575566</td>\n",
              "      <td>0.505399</td>\n",
              "      <td>0.823500</td>\n",
              "      <td>11:23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.482419</td>\n",
              "      <td>0.444031</td>\n",
              "      <td>0.844750</td>\n",
              "      <td>11:22</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXcuywRGNv8P",
        "colab_type": "text"
      },
      "source": [
        "Normalization becomes especially important when using pretrained models. The pretrained model only knows how to work with data of the type that it has seen before. If the average pixel value was 0 in the data it was trained with, but your data has 0 as the minimum possible value of a pixel, then the model is going to be seeing something very different to what is intended."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXSpTYIoEOQD",
        "colab_type": "text"
      },
      "source": [
        "###Progressive Resizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBbPKOQiN96S",
        "colab_type": "text"
      },
      "source": [
        "Start training using small images, and end training using large images. Spending most of the epochs training with small images, helps training complete much faster. Completing training using large images makes the final accuracy much higher. This approach is called progressive resizing.\n",
        "\n",
        "Progressive resizing is another form of data augmentation. Therefore, we expect to see better generalization of your models that are trained with progressive resizing. To implement progressive resizing it is most convenient if we first create a get_dls function which takes an image size and a batch size as we did in the section before, and returns the DataLoaders. Now we can create DataLoaders with a small size and use fit_one_cycle in the usual way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB7lbIB_D-2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "9554f662-9d04-41c8-8e28-c5aee2946ea7"
      },
      "source": [
        "\n",
        "dls = get_dls(128, 128)\n",
        "learn = Learner(dls, xresnet50(), loss_func=CrossEntropyLossFlat(), \n",
        "                metrics=accuracy)\n",
        "learn.fit_one_cycle(4, 3e-3)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.316990</td>\n",
              "      <td>1.673635</td>\n",
              "      <td>0.483083</td>\n",
              "      <td>04:57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.904789</td>\n",
              "      <td>0.945283</td>\n",
              "      <td>0.666000</td>\n",
              "      <td>04:58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.641992</td>\n",
              "      <td>0.566225</td>\n",
              "      <td>0.808417</td>\n",
              "      <td>04:58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.488755</td>\n",
              "      <td>0.465645</td>\n",
              "      <td>0.839000</td>\n",
              "      <td>04:57</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mS9HyA9O8H-",
        "colab_type": "text"
      },
      "source": [
        "We can replace the DataLoaders inside the Learner, and fine-tune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6ykym66D-tf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "3ca1afc1-6915-4ccb-8843-a30bd5559161"
      },
      "source": [
        "learn.dls = get_dls(64, 224)\n",
        "learn.fine_tune(5, 1e-3)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.652872</td>\n",
              "      <td>0.627547</td>\n",
              "      <td>0.786333</td>\n",
              "      <td>11:22</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.526220</td>\n",
              "      <td>0.512269</td>\n",
              "      <td>0.820833</td>\n",
              "      <td>11:23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.497310</td>\n",
              "      <td>0.478509</td>\n",
              "      <td>0.832417</td>\n",
              "      <td>11:24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.423831</td>\n",
              "      <td>0.425962</td>\n",
              "      <td>0.854917</td>\n",
              "      <td>11:23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.342455</td>\n",
              "      <td>0.357712</td>\n",
              "      <td>0.876750</td>\n",
              "      <td>11:23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.319672</td>\n",
              "      <td>0.336151</td>\n",
              "      <td>0.882750</td>\n",
              "      <td>11:23</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zV60NvzKPDml",
        "colab_type": "text"
      },
      "source": [
        "We can see there is improvement in performance and training small dataset takes much less time compared to training the whole large dataset at a time. \n",
        "\n",
        "For transfer learning, progressive resizing may actually hurt performance. This is most likely to happen if your pretrained model was quite similar to your transfer learning task and dataset and was trained on similar-sized images, so the weights don't need to be changed much. In that case, training on smaller images may damage the pretrained weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "estvx0dBEZoU",
        "colab_type": "text"
      },
      "source": [
        "###Test Time Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_q2MiCVQEuJ",
        "colab_type": "text"
      },
      "source": [
        "We could try to apply data augmentation to the validation set. Up until now, we have only applied it on the training set; the validation set always gets the same images. But maybe we could try to make predictions for a few augmented versions of the validation set and average them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ymwuq5QQZBL",
        "colab_type": "text"
      },
      "source": [
        "Random cropping in fastai often crops the center part of the image and not focus on the edges for the purpose of data augmentation and this might lead to a problem. Instead of doing center crop for validation we can crop the different areas of the image and pass all of them to the model to make the predictions. Instead of using different crops we can use different values as in test time augmentation paramaters.\n",
        "\n",
        "Depending on the dataset, test time augmentation can result in dramatic improvements in accuracy. It does not change the time required to train at all, but will increase the amount of time required for validation or inference by the number of test-time-augmented images requested. By default, fastai will use the unaugmented center crop image plus four randomly augmented images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2gVGMu0Eb--",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b523afaf-a600-415b-9149-ee0ca5164f1e"
      },
      "source": [
        "preds,targs = learn.tta()\n",
        "accuracy(preds, targs).item()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "                background: #F44336;\n",
              "            }\n",
              "        </style>\n",
              "      <progress value='0' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      \n",
              "    </div>\n",
              "    \n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.890999972820282"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaWOGkT7SccG",
        "colab_type": "text"
      },
      "source": [
        "###Mixup\n",
        "\n",
        "Mixup is a powerful data augmentation technique which can lead to higher accuracy of our model when we dont have enough data to train our model and having no pretrained model similar to our dataset. The paper explains: \"While data augmentation consistently leads to improved generalization, the procedure is dataset-dependent, and thus requires the use of expert knowledge.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3yl54tuTIWd",
        "colab_type": "text"
      },
      "source": [
        "Mixup works as follows, for each image:\n",
        "\n",
        "- Select another image from your dataset at random.\n",
        "- Pick a weight at random.\n",
        "- Take a weighted average (using the weight from step 2) of the selected image - with your image; this will be your independent variable.\n",
        "- Take a weighted average (with the same weight) of this image's labels with your image's labels; this will be your dependent variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aBriUJsTRYG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "c9ebe1bc-d1a1-4625-958c-9fc6a6fec799"
      },
      "source": [
        "model = xresnet50()\n",
        "learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), \n",
        "                metrics=accuracy, cbs=MixUp())\n",
        "learn.fit_one_cycle(5, 3e-3)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>1.766482</td>\n",
              "      <td>1.701730</td>\n",
              "      <td>0.410917</td>\n",
              "      <td>05:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.459237</td>\n",
              "      <td>1.416896</td>\n",
              "      <td>0.512000</td>\n",
              "      <td>05:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.249006</td>\n",
              "      <td>0.801754</td>\n",
              "      <td>0.733000</td>\n",
              "      <td>05:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.113351</td>\n",
              "      <td>0.630541</td>\n",
              "      <td>0.794750</td>\n",
              "      <td>05:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.043892</td>\n",
              "      <td>0.533257</td>\n",
              "      <td>0.831667</td>\n",
              "      <td>05:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzDZb1rKDoGU",
        "colab_type": "text"
      },
      "source": [
        "###Cutout data Augmentation\n",
        "\n",
        "A function cutout() is used to randomly display (with a probability of p) black squares in an image (number and size between min and max), forcing the ConvNet network to consider the context and not just learning to recognize features in isolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycgw49lmCXm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model = xresnet50()\n",
        "# learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), \n",
        "#                 metrics=accuracy, cbs=cutout())\n",
        "# learn.fit_one_cycle(3, 3e-3)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo2EB4JjTddW",
        "colab_type": "text"
      },
      "source": [
        "###Label Smoothing\n",
        "\n",
        "In most of the categorical dataset, targets are in the form of one  hot encoded which is the model is trained to return 0 for all categories but one, for which it is trained to return 1. Even 0.999 is not \"good enough\", the model will get gradients and learn to predict activations with even higher confidence. This encourages overfitting and gives you at inference time a model that is not going to give meaningful probabilities: it will always say 1 for the predicted category even if it's not too sure, just because it was trained this way. This is very dangerous if the data is not properly labelled.\n",
        "\n",
        "Instead, we could replace all our 1s with a number a bit less than 1, and our 0s by a number a bit more than 0, and then train. This is called label smoothing. By encouraging your model to be less confident, label smoothing will make your training more robust, even if there is mislabeled data. The result will be a model that generalizes better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6cOYcHeTf1F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "b9881ab0-556d-439a-91cb-bd1137e4e1c8"
      },
      "source": [
        "model = xresnet50()\n",
        "learn = Learner(dls, model, loss_func=LabelSmoothingCrossEntropy(), \n",
        "                metrics=accuracy)\n",
        "learn.fit_one_cycle(5, 3e-3)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>2.272966</td>\n",
              "      <td>2.321267</td>\n",
              "      <td>0.492250</td>\n",
              "      <td>04:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.916819</td>\n",
              "      <td>2.148390</td>\n",
              "      <td>0.599417</td>\n",
              "      <td>04:59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.664524</td>\n",
              "      <td>1.668528</td>\n",
              "      <td>0.755000</td>\n",
              "      <td>04:58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.496097</td>\n",
              "      <td>1.465727</td>\n",
              "      <td>0.832583</td>\n",
              "      <td>04:58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.414968</td>\n",
              "      <td>1.400136</td>\n",
              "      <td>0.859667</td>\n",
              "      <td>04:58</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8AeUGsyVyG3",
        "colab_type": "text"
      },
      "source": [
        "We have applied all the methods to create the state of the art model in computer vision for the CIFAR 10 dataset. However more epochs are needed to see if the model is avoiding overfitting and results in high accuracy."
      ]
    }
  ]
}